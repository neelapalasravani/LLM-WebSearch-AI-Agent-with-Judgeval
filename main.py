# Guardrail: Block unsafe or malicious queries
def is_safe_query(query: str) -> bool:
    unsafe_keywords = ["delete", "drop", "remove", "hack", "bypass", "shutdown", "malware", "attack", "override", "private data", "password"]
    return not any(word in query.lower() for word in unsafe_keywords)


import os
from dotenv import load_dotenv
import requests

from judgeval.tracer import Tracer
from judgeval import JudgmentClient
from judgeval.data import Example
from judgeval.scorers import FaithfulnessScorer

from langchain_together import ChatTogether
from langchain_core.messages.ai import AIMessage


# Load API Keys from .env

load_dotenv()  # Assumes .env is in the same directory

judgment = Tracer(project_name="llm-agent-sravani-v2")  #project name to avoid limit
client = JudgmentClient()

tavily_api_key = os.getenv("TAVILY_API_KEY")
together_api_key = os.getenv("TOGETHER_API_KEY")

# Debug: Confirm API keys loaded
assert tavily_api_key, "Tavily API key not found in environment"
assert together_api_key, "Together AI API key not found in environment"


# Tavily Web Search (Traced)

@judgment.observe(span_type="tool")
def search_tavily(query: str) -> str:
    url = "https://api.tavily.com/search"
    headers = {"Authorization": f"Bearer {tavily_api_key}"}
    params = {
        "query": query
    }
    try:
        response = requests.post(url, headers=headers, json=params)
        if response.status_code != 200:
            print(f"âš ï¸ Tavily API error: Status {response.status_code}")
            print(f"âš ï¸ Tavily error response: {response.text}")
            return f"Tavily search error: Status {response.status_code} - {response.text}"
        data = response.json()
        results = data.get("results", [])
        return results[0].get("content", "No content.") if results else "No results found."
    except Exception as e:
        print(f"âš ï¸ Tavily exception: {e}")
        return f"Tavily search exception: {e}"


# Together AI LLM Call (Traced)

@judgment.observe(span_type="tool")
def run_llm(question: str, context: str = "") -> str:
    try:
        llm = ChatTogether(
            together_api_key=together_api_key,
            model="meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
        )
        prompt = (
            "You are an expert research assistant. "
            "Use the following context to answer the question. "
            "Explain your reasoning step-by-step, cite sources if possible, and be concise. "
            f"\n\nContext: {context}\n\nQuestion: {question}\n\nReasoning:"
        )
        response = llm.invoke(prompt)
        if isinstance(response, str):
            return response
        elif isinstance(response, dict):
            return response.get("output", "No output from LLM.")
        elif isinstance(response, AIMessage):
            return response.content
        else:
            return f"Unexpected LLM response type: {type(response)}"
    except Exception as e:
        return f"LLM error: {e}"


# Main Agent Logic (Traced)

@judgment.observe(span_type="function")
def run_agent(question: str) -> tuple[str, str]:
    if not is_safe_query(question):
        return "âš ï¸ Query blocked by guardrail: Unsafe or malicious input detected.", "No context (blocked)"
    context = search_tavily(question)
    answer = run_llm(question, context)
    return answer, context


# Main Execution

if __name__ == "__main__":
    question = input("â“ Ask a web search question: ")
    answer, context = run_agent(question)

    print("\nğŸ“š Agent's Answer:")
    print(answer)

    # ğŸ” Basic Evaluation
    reference = input("\nğŸ’¡ What answer were you expecting (for eval)?: ")
    if not answer:
        print("\nâš ï¸ No answer was generated by the agent.")
    else:
        match = reference.strip().lower() in answer.strip().lower()
        print("\nğŸ§ª Basic Evaluation (String Containment):", "âœ… Match" if match else "âŒ No Match")

    # ğŸ§  LLM-based Evaluation (Faithfulness)
    try:
        example = Example(
            input=question,
            actual_output=answer,
            retrieval_context=[context]
        )
        scorer = FaithfulnessScorer(threshold=0.5)
        print("\nğŸ¤– Running LLM-based evaluation (GPT-4.1)...")
        client.assert_test(
            examples=[example],
            scorers=[scorer],
            model="gpt-4.1"
        )
    except Exception as e:
        print(f"\nâš ï¸ Advanced evaluation failed: {e}")